# -*- coding: utf-8 -*-
"""chatbot97lastoneISA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k9mfV6QOSCDiCVLAxWsrWYaHQC5USbcZ
"""

!pip install googletrans==4.0.0-rc1

import json
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional, BatchNormalization, Attention
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
import random
import nltk
from nltk.corpus import wordnet, stopwords
from transformers import BertTokenizer, TFBertForSequenceClassification
import googletrans

# Load the intents file
with open('intentsNEW2.json') as file:
    data = json.load(file)

# Preparing data
tags = []
patterns = []
responses = {}

for intent in data['intents']:
    for pattern in intent['patterns']:
        patterns.append(pattern)
        tags.append(intent['tag'])
    responses[intent['tag']] = intent['responses']

# Data Augmentation: Synonym Replacement
nltk.download('wordnet')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def synonym_replacement(sentence):
    words = sentence.split()
    new_sentence = []
    for word in words:
        if word in stop_words:
            new_sentence.append(word)
            continue
        synonyms = wordnet.synsets(word)
        if synonyms:
            synonym = synonyms[0].lemmas()[0].name()
            new_sentence.append(synonym)
        else:
            new_sentence.append(word)
    return ' '.join(new_sentence)

# Additional Data Augmentation: Random Insertion, Swap, and Deletion
def random_insertion(sentence):
    words = sentence.split()
    new_words = words.copy()
    for _ in range(3):  # Insert 3 random synonyms
        synonym = synonym_replacement(random.choice(words))
        if synonym:
            new_words.insert(random.randint(0, len(new_words)), synonym)
    return ' '.join(new_words)

def random_swap(sentence):
    words = sentence.split()
    if len(words) < 2:
        return sentence  # Return the original sentence if there are fewer than 2 words to swap
    new_words = words.copy()
    for _ in range(3):  # Swap 3 times
        idx1, idx2 = random.sample(range(len(new_words)), 2)
        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]
    return ' '.join(new_words)

def random_deletion(sentence, p=0.3):
    words = sentence.split()
    if len(words) == 1:
        return sentence
    new_words = [word for word in words if random.uniform(0, 1) > p]
    if len(new_words) == 0:
        return words[random.randint(0, len(words)-1)]
    return ' '.join(new_words)

# Back-Translation using Google Translate API
def back_translation(sentence, src_lang='en', mid_lang='fr'):
    translator = googletrans.Translator()
    translation = translator.translate(sentence, src=src_lang, dest=mid_lang)
    back_translation = translator.translate(translation.text, src=mid_lang, dest=src_lang)
    return back_translation.text

augmented_patterns = patterns.copy()
augmented_tags = tags.copy()
for pattern, tag in zip(patterns, tags):
    augmented_patterns.append(synonym_replacement(pattern))
    augmented_patterns.append(random_insertion(pattern))
    augmented_patterns.append(random_swap(pattern))
    augmented_patterns.append(random_deletion(pattern))
    augmented_patterns.append(back_translation(pattern))
    augmented_tags.extend([tag]*5)

# Encode the labels
label_encoder = LabelEncoder()
encoded_tags = label_encoder.fit_transform(augmented_tags)

# Tokenize the patterns
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(augmented_patterns)
sequences = tokenizer.texts_to_sequences(augmented_patterns)
padded_sequences = pad_sequences(sequences, padding='post')

# Split data
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_tags, test_size=0.2, random_state=42)

# Load pre-trained GloVe embeddings
def load_glove_embeddings(dim=100):
    embeddings_index = {}
    with open(f'glove.6B.{dim}d.txt', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
    return embeddings_index

def create_embedding_matrix(tokenizer, embeddings_index, dim=100):
    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, dim))
    for word, i in tokenizer.word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
    return embedding_matrix

embedding_dim = 100
embeddings_index = load_glove_embeddings(embedding_dim)
embedding_matrix = create_embedding_matrix(tokenizer, embeddings_index, embedding_dim)

# Learning Rate Scheduler
def lr_scheduler(epoch, lr):
    if epoch > 10:
        lr = lr * tf.math.exp(-0.1)
    return lr

import json
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional, BatchNormalization, Input, Attention
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
import random
import nltk
from nltk.corpus import wordnet, stopwords
from transformers import BertTokenizer, TFBertForSequenceClassification

# Model Building
input_layer = Input(shape=(padded_sequences.shape[1],))
embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1,
                            output_dim=embedding_dim,
                            weights=[embedding_matrix],
                            input_length=padded_sequences.shape[1],
                            trainable=False)(input_layer)
bidirectional_gru_1 = Bidirectional(GRU(128, return_sequences=True))(embedding_layer)
batch_norm_1 = BatchNormalization()(bidirectional_gru_1)
dropout_1 = Dropout(0.5)(batch_norm_1)
bidirectional_gru_2 = Bidirectional(GRU(128, return_sequences=True))(dropout_1)

# Attention mechanism
query_value_attention_seq = Attention()([bidirectional_gru_2, bidirectional_gru_2])

bidirectional_gru_3 = Bidirectional(GRU(128))(query_value_attention_seq)
dense_1 = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(bidirectional_gru_3)
dropout_2 = Dropout(0.5)(dense_1)
output_layer = Dense(len(set(augmented_tags)), activation='softmax')(dropout_2)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])
model.summary()

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')
lr_schedule = LearningRateScheduler(lr_scheduler)

# Training the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test),
                    callbacks=[early_stopping, model_checkpoint, lr_schedule])

# Evaluate the model
model.load_weights('best_model.h5')
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
accuracy = accuracy_score(y_test, y_pred_classes)

print(f'Model Accuracy: {accuracy * 100:.2f}%')

# Save the model and tokenizer
model.save('skin_cancer_chatbot_model.h5')
with open('tokenizer.json', 'w') as f:
    f.write(tokenizer.to_json())
with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

# Function to get response
def get_response(text):
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=padded_sequences.shape[1], padding='post')
    predicted_tag = model.predict(padded_sequence)
    tag = label_encoder.inverse_transform([np.argmax(predicted_tag)])
    return np.random.choice(responses[tag[0]])

# Example usage
while True:
    user_input = input("You: ")
    if user_input.lower() == "quit":
        break
    response = get_response(user_input)
    print(f"Bot: {response}")